{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json, urllib2\n",
    "import os, glob, sys, time\n",
    "import nltk,re\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read 70k recipes from allrecipes.com into a list of dictionaries\n",
    "def read_data():\n",
    "    folder='recipes/'\n",
    "    files=pd.read_csv('all_files.txt',header=None)[0].values\n",
    "    k=0\n",
    "    recipes=[]\n",
    "    st=time.time()\n",
    "    for filename in files:\n",
    "        f=open(folder+filename,'r')\n",
    "        r=json.load(f)\n",
    "        recipes.append(r)\n",
    "        k+=1\n",
    "        if k%10000==0:\n",
    "            print k\n",
    "    return recipes\n",
    "\n",
    "#Removing and replacing some noizy symbols\n",
    "def clean_string(s):    \n",
    "    sep_symbols=[';']\n",
    "    for ss in sep_symbols:\n",
    "        s=s.replace(ss,'.')\n",
    "    for i in range(10):\n",
    "        s=s.replace('..','.')\n",
    "    bad_symbols=[')','(','!','-']\n",
    "    for bs in bad_symbols:\n",
    "        s=s.replace(bs,' ')\n",
    "    s=s.replace(',',' , ')                  \n",
    "    s=s.replace('  ',' ')\n",
    "    s=s.replace('. ','.')\n",
    "    return s\n",
    "\n",
    "#Raw direction text -> List of single directions\n",
    "def get_clean_directions(recipe):\n",
    "    raw=recipe['directions']\n",
    "    direction=''\n",
    "    for dd in raw:\n",
    "        direction=direction+dd+'.'\n",
    "    direction=clean_string(direction).lower()\n",
    "    s=direction.split('.')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlighting recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bright(l,idx1,idx2,idx3):\n",
    "    l2=[]\n",
    "    for i in range(len(l)):\n",
    "        if idx1[i]:\n",
    "            l2.append(yellow(l[i]))\n",
    "        elif idx2[i]:\n",
    "            l2.append(blue(l[i]))\n",
    "        elif idx3[i]:\n",
    "            l2.append(purple(l[i]))\n",
    "        else:\n",
    "            l2.append(l[i])\n",
    "    l2=' '.join(l2)\n",
    "    return l2\n",
    "\n",
    "def purple(string):\n",
    "    return '\\x1b[1;45m'+string+'\\x1b[0m'\n",
    "\n",
    "def yellow(string):\n",
    "    return '\\x1b[1;43m'+string+'\\x1b[0m'\n",
    "\n",
    "def blue(string):\n",
    "    return '\\x1b[1;46m'+string+'\\x1b[0m'\n",
    "\n",
    "def highlight_recipe(recipes,recipe_id):\n",
    "    dirs=get_clean_directions(recipes[recipe_id])\n",
    "    ingr_words=list(set(ingr_words_func(recipes[recipe_id]['ingr'])))\n",
    "    for d in dirs:\n",
    "        if len(d)>0:\n",
    "            d_words=np.array(d.split(' '))\n",
    "            ingr_idx,measure_idx=np.array(define_ingr_measure(d_words, ingr_words))\n",
    "            action_idx=np.array([(word in actions_set) for word in d_words]).astype(np.int32)\n",
    "            colored_string=bright(d_words,action_idx,ingr_idx,measure_idx)\n",
    "            print colored_string\n",
    "            print create_instructions(d)\n",
    "            print '_____________________________________'\n",
    "            #print create_instructions(d)\n",
    "            \n",
    "\n",
    "            \n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning and reading ingridiends and measures\n",
    "\n",
    "def read_measure_list(path):\n",
    "    measures=pd.read_csv(path,header=None)\n",
    "    measure_list=measures[0].values\n",
    "    return measure_list\n",
    "\n",
    "def remove_stopwords(text_list):\n",
    "    stop = stopwords.words('english')\n",
    "    content = [w for w in text_list if w.lower() not in stop]\n",
    "    return content\n",
    "\n",
    "def remove_digits(text_list):\n",
    "    content=[]\n",
    "    for w in text_list:\n",
    "        w = re.sub('[./]', ' ', w).split() \n",
    "        content.append(w)\n",
    "    content = list(itertools.chain.from_iterable(content))\n",
    "    content = [w for w in content if w.isdigit()==0]\n",
    "    return content\n",
    "\n",
    "def get_clean_text(text):\n",
    "    return text.replace('(','').replace(')','').replace(',','').replace('-',' ').replace('/',' ').replace(';',' ').replace('  ',' ')\n",
    "\n",
    "def ingr_words_func(ingr_list):\n",
    "    recipe_words=[]\n",
    "    for recipe in ingr_list:\n",
    "        recipe=get_clean_text(recipe)\n",
    "        recipe_words.append([element for element in recipe.lower().split()])\n",
    "    recipe_words = list(itertools.chain.from_iterable(recipe_words))\n",
    "    recipe_words=remove_stopwords(remove_digits(recipe_words))\n",
    "    return recipe_words\n",
    "\n",
    "#defining ingridients and measures\n",
    "def define_ingr_measure(dirs_words, ingr_words):\n",
    "    if_ingr=[0]*len(dirs_words)\n",
    "    if_measure=[0]*len(dirs_words)\n",
    "    for i,dirs_word in enumerate(dirs_words):\n",
    "        for ingrs in ingr_words:\n",
    "            if dirs_word==ingrs:\n",
    "                if dirs_word not in measure_list:\n",
    "                    if_ingr[i]=1\n",
    "                else:\n",
    "                    if_measure[i]=1\n",
    "    return if_ingr,if_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic role labeling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from practnlptools.tools import Annotator\n",
    "annotator=Annotator()\n",
    "def create_instructions(phrase,flag=1):\n",
    "    \n",
    "    annotated = annotator.getAnnotations(phrase)['srl']\n",
    "    annotated_steps = []\n",
    "    if len(annotated) > 0:\n",
    "        for i in xrange(len(annotated)):\n",
    "            annotated_step = dict()\n",
    "            annotated_step['action'] = annotated[i]['V']\n",
    "            if set(['A1','A2']).issubset(annotated[i].keys()):\n",
    "                annotated_step['object'] = annotated[i]['A1']\n",
    "                annotated_step['target'] = annotated[i]['A2']\n",
    "            elif set(['A2']).issubset(annotated[i].keys()):\n",
    "                annotated_step['target'] = annotated[i]['A2']\n",
    "            elif set(['A1']).issubset(annotated[i].keys()):\n",
    "                annotated_step['object'] = annotated[i]['A1']\n",
    "            else:\n",
    "                pass\n",
    "            annotated_steps.append(annotated_step)\n",
    "    if (len(annotated_steps)==0) & (flag):\n",
    "        return create_instructions('they '+phrase,0)\n",
    "    return annotated_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "CPU times: user 10.1 s, sys: 12.1 s, total: 22.3 s\n",
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%time recipes=read_data()\n",
    "actions=pd.read_csv('actions_dict_sorted.txt',sep=' ')\n",
    "measure_list=read_measure_list('measure_list.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions_set=set(actions[:100].word.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13388"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes[6710]['recipe_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1/2 cup chopped red onion',\n",
       " u'1/4 cup chopped celery',\n",
       " u'1 teaspoon garlic powder',\n",
       " u'2 tablespoons margarine',\n",
       " u'1/2 cup all-purpose flour',\n",
       " u'4 cups chicken broth',\n",
       " u'1 1/2 cups chopped baby carrots',\n",
       " u'2 potatoes, peeled and diced',\n",
       " u'1 tablespoon chopped fresh parsley',\n",
       " u'1 teaspoon freshly ground black pepper',\n",
       " u'1 pinch chopped fresh dill weed',\n",
       " u'3 cups milk',\n",
       " u'3 cups shredded Cheddar cheese']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes[6718]['ingr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using a large stock pot , \u001b[1;43msaute\u001b[0m onions , \u001b[1;46mcelery\u001b[0m and \u001b[1;46mgarlic\u001b[0m \u001b[1;46mpowder\u001b[0m in butter , over \u001b[1;43mlow\u001b[0m \u001b[1;43mheat\u001b[0m until onions \u001b[1;43mare\u001b[0m tender\n",
      "[{'action': 'using', 'object': 'a large stock pot , saute onions , celery and garlic powder in butter'}]\n",
      "_____________________________________\n",
      "slowly \u001b[1;43mstir\u001b[0m in \u001b[1;43mflour\u001b[0m with 1 \u001b[1;45mcup\u001b[0m of \u001b[1;46mchicken\u001b[0m \u001b[1;46mbroth\u001b[0m to \u001b[1;43mmake\u001b[0m a rue\n",
      "[{'action': 'stir', 'object': 'with 1 cup of chicken broth', 'target': 'in flour'}, {'action': 'make', 'object': 'a rue'}]\n",
      "_____________________________________\n",
      "\u001b[1;43mstir\u001b[0m until well mixed\n",
      "[{'action': 'stir', 'target': 'until well mixed'}]\n",
      "_____________________________________\n",
      "\u001b[1;43madd\u001b[0m 3 \u001b[1;45mcups\u001b[0m \u001b[1;46mchicken\u001b[0m \u001b[1;46mbroth\u001b[0m , \u001b[1;46mcarrots\u001b[0m , \u001b[1;46mpotatoes\u001b[0m , \u001b[1;46mparsley\u001b[0m , \u001b[1;46mpepper\u001b[0m and \u001b[1;46mdill\u001b[0m\n",
      "[{'action': 'add', 'object': '3 cups chicken broth , carrots , potatoes , parsley , pepper and dill'}]\n",
      "_____________________________________\n",
      "\u001b[1;43mbring\u001b[0m to a \u001b[1;43mboil\u001b[0m and then \u001b[1;43madd\u001b[0m \u001b[1;46mmilk\u001b[0m and \u001b[1;46mcheese\u001b[0m\n",
      "[{'action': 'bring'}, {'action': 'add', 'object': 'milk and cheese'}]\n",
      "_____________________________________\n",
      "\u001b[1;43mstir\u001b[0m until \u001b[1;46mcheese\u001b[0m is melted , \u001b[1;43mreduce\u001b[0m \u001b[1;43mheat\u001b[0m to \u001b[1;43mlow\u001b[0m and \u001b[1;43msimmer\u001b[0m for one hour , stirring occasionally\n",
      "[{'action': 'melted', 'object': 'cheese'}, {'action': 'reduce', 'object': 'heat', 'target': 'to low'}, {'action': 'simmer'}, {'action': 'stirring'}]\n",
      "_____________________________________\n"
     ]
    }
   ],
   "source": [
    "highlight_recipe(recipes,6718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_id</th>\n",
       "      <th>action</th>\n",
       "      <th>object</th>\n",
       "      <th>target</th>\n",
       "      <th>no</th>\n",
       "      <th>nt</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>heat</td>\n",
       "      <td>oil</td>\n",
       "      <td>skillet</td>\n",
       "      <td>1 1/2 tablespoon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>add</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>body</td>\n",
       "      <td>1/4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>add</td>\n",
       "      <td>pork</td>\n",
       "      <td>body</td>\n",
       "      <td>4 ounce</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>fry</td>\n",
       "      <td>body</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>until pork in no longer pink inside approximat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>stir</td>\n",
       "      <td>body</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>while it is frying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>add</td>\n",
       "      <td>chicken broth</td>\n",
       "      <td>body</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>add</td>\n",
       "      <td>soy sause</td>\n",
       "      <td>body</td>\n",
       "      <td>2 tablespoon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>add</td>\n",
       "      <td>ginger</td>\n",
       "      <td>body</td>\n",
       "      <td>1/2 teaspoon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>bring</td>\n",
       "      <td>body</td>\n",
       "      <td>boil</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>reduce</td>\n",
       "      <td>heat</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>simmer</td>\n",
       "      <td>body</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10 minutes  stirring occasionally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>stir</td>\n",
       "      <td>onion</td>\n",
       "      <td>body</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>add</td>\n",
       "      <td>noodles</td>\n",
       "      <td>body</td>\n",
       "      <td>4 ounce</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>cook</td>\n",
       "      <td>body</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>until noodles are tender 2 to 4 minutes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    line_id  action         object   target                no  nt  \\\n",
       "0         0    heat            oil  skillet  1 1/2 tablespoon   0   \n",
       "1         1     add        cabbage     body               1/4   0   \n",
       "2         1     add           pork     body           4 ounce   0   \n",
       "3         2     fry           body        0                 0   0   \n",
       "4         3    stir           body        0                 0   0   \n",
       "5         4     add  chicken broth     body                 6   0   \n",
       "6         4     add      soy sause     body      2 tablespoon   0   \n",
       "7         4     add         ginger     body      1/2 teaspoon   0   \n",
       "8         4   bring           body     boil                 0   0   \n",
       "9         5  reduce           heat      low                 0   0   \n",
       "10        6  simmer           body        0                 0   0   \n",
       "11        7    stir          onion     body                 8   0   \n",
       "12        7     add        noodles     body           4 ounce   0   \n",
       "13        8    cook           body        0                 0   0   \n",
       "\n",
       "                                          description  \n",
       "0                                                   0  \n",
       "1                                                   0  \n",
       "2                                                   0  \n",
       "3   until pork in no longer pink inside approximat...  \n",
       "4                                  while it is frying  \n",
       "5                                                   0  \n",
       "6                                                   0  \n",
       "7                                                   0  \n",
       "8                                                   0  \n",
       "9                                                   0  \n",
       "10                  10 minutes  stirring occasionally  \n",
       "11                                                  0  \n",
       "12                                                  0  \n",
       "13            until noodles are tender 2 to 4 minutes  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('labeled_recipes/l13394.txt')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l6665.txt\n",
      "l6667.txt\n",
      "l6668.txt\n",
      "l6669.txt\n",
      "l6670.txt\n",
      "l6671.txt\n",
      "l6673.txt\n",
      "l6674.txt\n",
      "l6675.txt\n",
      "l6676.txt\n",
      "l6677.txt\n",
      "l6678.txt\n",
      "l6679.txt\n",
      "l6680.txt\n",
      "l6681.txt\n",
      "l6682.txt\n",
      "l6683.txt\n",
      "l6684.txt\n",
      "l6685.txt\n",
      "l6686.txt\n",
      "l6688.txt\n",
      "l6689.txt\n",
      "l6690.txt\n",
      "l6691.txt\n",
      "l6692.txt\n",
      "l6693.txt\n",
      "l6694.txt\n",
      "l6698.txt\n",
      "l6699.txt\n",
      "l6700.txt\n",
      "l6701.txt\n",
      "l6702.txt\n",
      "l6703.txt\n",
      "l6704.txt\n",
      "l6705.txt\n",
      "l6706.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory = \"labeled_recipes\"\n",
    "for filename in os.listdir(directory)[2:]:\n",
    "    idx = int(filename.split('.')[0][1:])\n",
    "    new_filename = os.path.join(directory,'l'+ str(recipes[idx]['recipe_id']) + '.txt')\n",
    "    os.rename(os.path.join(directory,filename),new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 'heat', 'oil', 'pot', '6 tablespoon', 0, '0'],\n",
       " [0, 'add', 'tortillas', 'body', '8 (6 inch)', 0, '0'],\n",
       " [0, 'add', 'garlic', 'body', '6 clove', 0, '0'],\n",
       " [0, 'add', 'cilantro', 'body', '1/2 cup', 0, '0'],\n",
       " [0, 'add', 'onion', 'body', '1', 0, '0'],\n",
       " [1, 'saute', 'body', '0', '0', 0, ' for 2 to 3 minutes'],\n",
       " [2, 'stir', 'tomatoes', 'body', '1 (29 ounce)', 0, '0'],\n",
       " [2, 'bring', 'body', 'boil', '0', 0, '0'],\n",
       " [3, 'add', 'cumin', 'body', '2 tablespoon', 0, '0'],\n",
       " [3, 'add', 'chili powder', 'body', '1 tablespoon', 0, '0'],\n",
       " [3, 'add', 'bay leaves', 'body', '3', 0, '0'],\n",
       " [3, 'add', 'chicken', 'body', '6 cup', 0, '0'],\n",
       " [4, 'return', 'body', 'boil', '0', 0, '0'],\n",
       " [4, 'reduce', 'heat', 'medium', '0', 0, '0'],\n",
       " [4, 'add', 'salt', 'body', '1 teaspoon', 0, '0'],\n",
       " [4, 'add', 'cayenne', 'body', '1/2 teaspoon', 0, '0'],\n",
       " [5, 'simmer', 'body', '0', '0', 0, 'for 30 minutes'],\n",
       " [5, 'remove', 'bay leaves', '0', '0', 0, '0'],\n",
       " [5, 'stir', 'chicken', 'body', '0', 0, '0'],\n",
       " [6, 'heat', 'body', '0', '0', 0, '0'],\n",
       " [6, 'serve', 'body', '0', '0', 0, '0']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df.values.tolist()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempt to build flow of commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in a large pan or wok , heat oil over medium high heat\n",
      "[]\n",
      "____________________________________________\n",
      "stir in the garlic , ginger , and chile pepper\n",
      "[{'action': 'stir', 'target': 'in the garlic , ginger , and chile pepper'}]\n",
      "____________________________________________\n",
      "as the garlic and ginger begin to sweat , add beans , toss to mix , cover and reduce heat\n",
      "[{'action': 'begin', 'object': 'the garlic and ginger'}, {'action': 'sweat', 'object': 'the garlic and ginger'}, {'action': 'add', 'object': 'beans'}, {'action': 'toss', 'object': 'to mix'}, {'action': 'mix'}, {'action': 'reduce', 'object': 'heat'}]\n",
      "____________________________________________\n",
      "steam for 5 8 minutes\n",
      "[]\n",
      "____________________________________________\n",
      "add 1/4 cup of water if necessary\n",
      "[{'action': 'add', 'object': '1/4 cup of water'}]\n",
      "____________________________________________\n",
      "remove cover , increase heat to high\n",
      "[{'action': 'remove', 'object': 'cover , increase heat to high'}]\n",
      "____________________________________________\n",
      "add soy and oyster sauce and stir for two minutes more or until sauce thickens\n",
      "[{'action': 'add', 'object': 'soy and oyster sauce'}, {'action': 'stir'}, {'action': 'thickens', 'object': 'sauce'}]\n",
      "____________________________________________\n",
      "serve warm\n",
      "[{'action': 'serve', 'target': 'warm'}]\n",
      "____________________________________________\n"
     ]
    }
   ],
   "source": [
    "recipe_id=7777\n",
    "dirs=get_clean_directions(recipes[recipe_id])\n",
    "ingr_words=list(set(ingr_words_func(recipes[recipe_id]['ingr'])))\n",
    "for d in dirs:\n",
    "    if len(d)>0:\n",
    "        print d\n",
    "        print create_instructions(d)\n",
    "        print '____________________________________________'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayseian approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def is_known(word):\n",
    "    \"\"\"return True if this word \"exists\" in WordNet\n",
    "       (or at least in nltk.corpus.stopwords).\"\"\"\n",
    "    if word.lower() in nltk.corpus.stopwords.words('english'):\n",
    "        return True\n",
    "    synset = wn.synsets(word,pos=wn.VERB)\n",
    "    if len(synset) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actions_dict_upd = dict()\n",
    "actions_dict = dict(actions.values.tolist())\n",
    "word_actions = actions_dict.keys()\n",
    "for word in word_actions:\n",
    "    if is_known(word) == True:\n",
    "        actions_dict_upd[word] = actions_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_x = sorted(actions_dict_upd.items(), key=operator.itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('action_dict_wordnet.txt','w')\n",
    "for (x,y) in sorted_x:\n",
    "    if y > 40:\n",
    "        f.write(x + ' ' + str(y) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== #-grams = 2 ====\n",
      "(u'350', u'175') 3.99425974384\n",
      "(u'f', u'175') 3.97415922752\n",
      "(u'degrees', u'175') 3.9454442042\n",
      "(u'and', u'beat') 3.79038307827\n",
      "(u'plastic', u'wrap') 2.99677187865\n",
      "(u'375', u'190') 2.99677187865\n",
      "(u'smooth', u'elastic') 2.99461902529\n",
      "(u'baking', u'soda') 2.98816046521\n",
      "(u'cover', u'plastic') 2.98170190512\n",
      "(u'bake', u'at') 2.98170190512\n",
      "(u'f', u'190') 2.97954905176\n",
      "(u'with', u'plastic') 2.9666319316\n",
      "(u'with', u'wrap') 2.9666319316\n",
      "(u'with', u'towel') 2.9666319316\n",
      "(u'let', u'stand') 2.96232622488\n",
      "(u'degrees', u'190') 2.95802051815\n",
      "(u'until', u'combined') 2.93433913118\n",
      "(u'and', u'elastic') 2.84176643665\n",
      "(u'away', u'sides') 1.99856562259\n",
      "(u'pulls', u'away') 1.99856562259\n",
      "(u'tester', u'inserted') 1.99856562259\n",
      "(u'7', u'tablespoons') 1.99713090212\n",
      "(u'after', u'addition') 1.99569618164\n",
      "(u'from', u'sides') 1.99569618164\n",
      "(u'each', u'addition') 1.99282674069\n",
      "(u'loaves', u'are') 1.99282674069\n",
      "(u'eggs', u'orange') 1.99139202022\n",
      "(u'floured', u'counter') 1.99139202022\n",
      "(u'eggs', u'juice') 1.99139202022\n",
      "(u'pan', u'cornbread') 1.98995729974\n",
      "(u'bake', u'preheated') 1.98708785879\n",
      "(u'2', u'cup') 1.98565313831\n",
      "(u'well', u'addition') 1.98565313831\n",
      "(u'with', u'hands') 1.97704481546\n",
      "(u'bowl', u'sift') 1.97561009498\n",
      "(u'1', u'cup') 1.97561009498\n",
      "(u'1', u'tablespoon') 1.97561009498\n",
      "(u'let', u'sit') 1.97417537451\n",
      "(u'let', u'covered') 1.97417537451\n",
      "(u'add', u'whole') 1.97130593356\n",
      "(u'a', u'counter') 1.95982816975\n",
      "(u'until', u'doubles') 1.95552400832\n",
      "(u'until', u'pulls') 1.95552400832\n",
      "(u'until', u'tester') 1.95552400832\n",
      "(u'until', u'away') 1.95552400832\n",
      "(u'to', u'ingredients') 1.95408928785\n",
      "(u'in', u'preheated') 1.94261152404\n",
      "(u'the', u'milk') 1.93400320119\n",
      "(u'the', u'sides') 1.93400320119\n",
      "(u'punch', u'down') 1.49224350438\n",
      "(u'golden', u'brown') 0.745993232362\n",
      "(u'floured', u'surface') 0.741685981084\n",
      "(u'let', u'rest') 0.724456975971\n",
      "(u'minutes', u'cool') 0.711535222136\n",
      "(u'a', u'surface') 0.71009947171\n",
      "(u'in', u'size') 0.692870466596\n",
      "(u'and', u'9') 0.644054952109\n",
      "(u'and', u'rest') 0.644054952109\n",
      "(u'loaf', u'pans') 0.173729702741\n",
      "(u'into', u'loaves') 0.153614756752\n",
      "(u'comes', u'clean') 0.0890220882527\n",
      "(u'whole', u'wheat') 0.0890220882527\n",
      "(u'margarine', u'hot') 0.0890220882527\n",
      "(u'9', u'x') 0.0875868531713\n",
      "(u'preheat', u'375') 0.0868692356307\n",
      "(u'each', u'sheet') 0.08615161809\n",
      "(u'large', u'mixing') 0.0854340005493\n",
      "(u'baking', u'sheets') 0.0854340005493\n",
      "(u'large', u'7') 0.0854340005493\n",
      "(u'out', u'clean') 0.0847163830087\n",
      "(u'is', u'elastic') 0.083998765468\n",
      "(u'salt', u'soda') 0.083998765468\n",
      "(u'oven', u'375') 0.0832811479273\n",
      "(u'c', u'grease') 0.0832811479273\n",
      "(u'cover', u'wrap') 0.0832811479273\n",
      "(u'rise', u'again') 0.0825635303867\n",
      "(u'place', u'on') 0.0825635303867\n",
      "(u'bowl', u'7') 0.077540207602\n",
      "(u'degrees', u'grease') 0.0753873549801\n",
      "(u'a', u'towel') 0.0696464146547\n",
      "(u'a', u'small') 0.0696464146547\n",
      "(u'a', u'mixing') 0.0696464146547\n",
      "(u'to', u'375') 0.0667759444921\n",
      "(u'to', u'make') 0.0667759444921\n",
      "(u'in', u'small') 0.0610350041668\n",
      "(u'in', u'medium') 0.0610350041668\n",
      "(u'the', u'liquid') 0.0567292989228\n",
      "(u'and', u'soda') 0.0366360077842\n",
      "(u'and', u'stand') 0.0366360077842\n",
      "==== #-grams = 3 ====\n",
      "(u'350', u'f', u'175') 3.99805997888\n",
      "(u'350', u'degrees', u'175') 3.99805997888\n",
      "(u'375', u'degrees', u'190') 2.99890882635\n",
      "(u'375', u'f', u'190') 2.99890882635\n",
      "(u'smooth', u'and', u'elastic') 2.99890882635\n",
      "(u'with', u'plastic', u'wrap') 2.99890882635\n",
      "(u'cover', u'plastic', u'wrap') 2.99890882635\n",
      "(u'from', u'the', u'sides') 1.99951507314\n",
      "(u'away', u'from', u'sides') 1.99951507314\n",
      "(u'out', u'floured', u'counter') 1.99951507314\n",
      "(u'pulls', u'from', u'sides') 1.99951507314\n",
      "(u'well', u'after', u'addition') 1.99951507314\n",
      "(u'pulls', u'away', u'sides') 1.99951507314\n",
      "(u'flour', u'after', u'addition') 1.99951507314\n",
      "(u'flour', u'well', u'addition') 1.99951507314\n",
      "(u'until', u'tester', u'inserted') 1.99951507314\n",
      "(u'out', u'a', u'counter') 1.99951507314\n",
      "(u'well', u'each', u'addition') 1.99951507314\n",
      "(u'away', u'the', u'sides') 1.99951507314\n",
      "(u'until', u'loaves', u'are') 1.99951507314\n",
      "(u'after', u'each', u'addition') 1.99951507314\n",
      "(u'pulls', u'the', u'sides') 1.99951507314\n",
      "(u'as', u'bread', u'easily') 1.99951507314\n",
      "(u'tester', u'inserted', u'comes') 1.99951507314\n",
      "(u'until', u'pulls', u'away') 1.99951507314\n",
      "(u'5', u'minutes', u'creamy') 1.99903010707\n",
      "(u'dissolve', u'the', u'milk') 1.99903010707\n",
      "(u'bake', u'in', u'preheated') 1.99903010707\n",
      "(u'dough', u'smooth', u'elastic') 0.0899724506468\n",
      "(u'with', u'a', u'towel') 0.0899724506468\n",
      "(u'is', u'smooth', u'elastic') 0.0899724506468\n",
      "(u'comes', u'out', u'clean') 0.0899724506468\n",
      "(u'oven', u'375', u'190') 0.0899724506468\n",
      "(u'to', u'375', u'190') 0.0899724506468\n",
      "(u'2', u'3', u'more') 0.0899724506468\n",
      "(u'cover', u'a', u'towel') 0.0899724506468\n",
      "(u'is', u'and', u'elastic') 0.0899724506468\n",
      "(u'salt', u'baking', u'soda') 0.0899724506468\n",
      "(u'flour', u'baking', u'soda') 0.0899724506468\n",
      "(u'add', u'whole', u'wheat') 0.0899724506468\n",
      "(u'until', u'smooth', u'elastic') 0.0899724506468\n",
      "(u'f', u'190', u'grease') 0.0897299382169\n",
      "(u'190', u'degrees', u'grease') 0.0897299382169\n",
      "(u'dough', u'is', u'elastic') 0.0897299382169\n",
      "(u'bowl', u'with', u'plastic') 0.0897299382169\n",
      "(u'190', u'c', u'grease') 0.0897299382169\n",
      "(u'and', u'beat', u'combined') 0.0894874257869\n",
      "(u'until', u'and', u'elastic') 0.0894874257869\n",
      "(u'dough', u'and', u'elastic') 0.0894874257869\n",
      "(u'bowl', u'cover', u'plastic') 0.0894874257869\n",
      "(u'2', u'cups', u'more') 0.0894874257869\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import operator\n",
    "import nltk\n",
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def isValid(word):\n",
    "  if word.startswith(\"#\"):\n",
    "    return False # no hashtag\n",
    "  else:\n",
    "    vword = word.translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "    return len(vword) == len(word)\n",
    "\n",
    "def llr(c1, c2, c12, n):\n",
    "  # H0: Independence p(w1,w2) = p(w1,~w2) = c2/N\n",
    "  p0 = c2 / n\n",
    "  # H1: Dependence, p(w1,w2) = c12/N\n",
    "  p10 = c12 / n\n",
    "  # H1: p(~w1,w2) = (c2-c12)/N\n",
    "  p11 = (c2 - c12) / n\n",
    "  # binomial probabilities\n",
    "  # H0: b(c12; c1, p0),  b(c2-c12; N-c1, p0)\n",
    "  # H1: b(c12, c1, p10), b(c2-c12; N-c1, p11)\n",
    "  probs = np.matrix([\n",
    "    [binom(c1, p0).logpmf(c12), binom(n - c1, p0).logpmf(c2 - c12)],\n",
    "    [binom(c1, p10).logpmf(c12), binom(n - c1, p11).logpmf(c2 - c12)]])\n",
    "  # LLR = p(H1) / p(H0)\n",
    "  return np.sum(probs[1, :]) - np.sum(probs[0, :])\n",
    "\n",
    "def isLikelyNGram(ngram, phrases):\n",
    "  if len(ngram) == 2:\n",
    "    return True\n",
    "  prevGram = ngram[:-1]\n",
    "  return phrases.has_key(prevGram)\n",
    "\n",
    "def main():\n",
    "  # accumulate words and word frequency distributions\n",
    "  lines = []\n",
    "  unigramFD = nltk.FreqDist()\n",
    "  \n",
    "  i = 0\n",
    "  for line in recipes[:10]:\n",
    "    i += 1\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(''.join(line['directions']).strip('.').lower())\n",
    "    #words = filter(lambda x: isValid(x), words)\n",
    "    for x in words:\n",
    "      unigramFD[x] += 1\n",
    "    lines.append(words)\n",
    "    if i > 1000:\n",
    "      break\n",
    "  \n",
    "  # identify likely phrases using a multi-pass algorithm based\n",
    "  # on the LLR approach described in the Building Search Applications\n",
    "  # Lucene, LingPipe and GATE book, except that we treat n-gram\n",
    "  # collocations beyond 2 as n-1 gram plus a unigram.\n",
    "  phrases = nltk.defaultdict(float)\n",
    "  prevGramFD = None\n",
    "  for i in range(2, 4):\n",
    "    ngramFD = nltk.FreqDist()\n",
    "    for words in lines:\n",
    "      nextGrams = nltk.skipgrams(words, i,i)\n",
    "      nextGrams = filter(lambda x: isLikelyNGram(x, phrases), nextGrams)\n",
    "      for x in nextGrams:\n",
    "        ngramFD[x] += 1\n",
    "    for k, v in ngramFD.iteritems():\n",
    "      if v > 1 and v < 5:\n",
    "        c1 = unigramFD[k[0]] if prevGramFD == None else prevGramFD[k[:-1]]\n",
    "        \n",
    "        c2 = unigramFD[k[1]] if prevGramFD == None else unigramFD[k[len(k) - 1]]\n",
    "        c12 = ngramFD[k]\n",
    "        n = unigramFD.N() if prevGramFD == None else prevGramFD.N()\n",
    "        phrases[k] = llr(c1, c2, c12, n)\n",
    "    # only consider bigrams where LLR > 0, ie P(H1) > P(H0)\n",
    "    likelyPhrases = nltk.defaultdict(float)\n",
    "    likelyPhrases.update([(k, v) for (k, v)\n",
    "      in phrases.iteritems() if len(k) == i and v > 0])\n",
    "    \n",
    "    print \"==== #-grams = %d ====\" % (i)\n",
    "    sortedPhrases = sorted(likelyPhrases.items(),\n",
    "      key=operator.itemgetter(1), reverse=True)\n",
    "    for k, v in sortedPhrases:\n",
    "        print k, v\n",
    "    prevGramFD = ngramFD\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "      main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', 'one two', 'two three', 'three four', 'one two three', 'two three four']\n"
     ]
    }
   ],
   "source": [
    "def word_grams(words, min=1, max=4):\n",
    "    s = []\n",
    "    for n in range(min, max):\n",
    "        for ngram in ngrams(words, n):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s\n",
    "\n",
    "print word_grams('one two three four'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Singularity',), ('is',), ('still',), ('a',), ('confusing',), ('phenomenon',), ('in',), ('physics',), ('Singularity', 'is'), ('is', 'still'), ('still', 'a'), ('a', 'confusing'), ('confusing', 'phenomenon'), ('phenomenon', 'in'), ('in', 'physics'), ('Singularity', 'is', 'still'), ('is', 'still', 'a'), ('still', 'a', 'confusing'), ('a', 'confusing', 'phenomenon'), ('confusing', 'phenomenon', 'in'), ('phenomenon', 'in', 'physics'), ('Singularity', 'is', 'still', 'a'), ('is', 'still', 'a', 'confusing'), ('still', 'a', 'confusing', 'phenomenon'), ('a', 'confusing', 'phenomenon', 'in'), ('confusing', 'phenomenon', 'in', 'physics'), ('Singularity', 'is', 'still', 'a', 'confusing'), ('is', 'still', 'a', 'confusing', 'phenomenon'), ('still', 'a', 'confusing', 'phenomenon', 'in'), ('a', 'confusing', 'phenomenon', 'in', 'physics'), ('Singularity', 'is', 'still', 'a', 'confusing', 'phenomenon'), ('is', 'still', 'a', 'confusing', 'phenomenon', 'in'), ('still', 'a', 'confusing', 'phenomenon', 'in', 'physics'), ('Singularity', 'is', 'still', 'a', 'confusing', 'phenomenon', 'in'), ('is', 'still', 'a', 'confusing', 'phenomenon', 'in', 'physics'), ('Singularity', 'is', 'still', 'a', 'confusing', 'phenomenon', 'in', 'physics')]\n"
     ]
    }
   ],
   "source": [
    "def everygrams(sequence,min_len=1, max_len=-1):\n",
    "    \"\"\"\n",
    "    This function returns all possible ngrams for n \n",
    "    ranging from 1 to len(sequence).\n",
    "    >>> list(everygrams('a b c'.split()))\n",
    "    [('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]\n",
    "    \"\"\"\n",
    "    for n in range(1, len(sequence)+1):\n",
    "        for ng in nltk.ngrams(sequence, n):\n",
    "            yield ng\n",
    "\n",
    "doc1 = \"Singularity is still a confusing phenomenon in physics\".split()\n",
    "doc2 = \"Quantum theory still wins over String theory\".split()\n",
    "_vec1 = list(everygrams(doc1,min_len=2, max_len=2))\n",
    "print _vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Insurgents', 'killed'),\n",
       " ('Insurgents', 'in'),\n",
       " ('Insurgents', 'ongoing'),\n",
       " ('Insurgents', 'fighting'),\n",
       " ('killed', 'in'),\n",
       " ('killed', 'ongoing'),\n",
       " ('killed', 'fighting'),\n",
       " ('in', 'ongoing'),\n",
       " ('in', 'fighting'),\n",
       " ('ongoing', 'fighting')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice, chain, combinations\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "def skipgrams(sequence, n, k):\n",
    "    \n",
    "    for ngram in nltk.ngrams(sequence, n + k, pad_right=True):\n",
    "        head = ngram[:1]\n",
    "        tail = ngram[1:]\n",
    "        for skip_tail in combinations(tail, n - 1):\n",
    "            if skip_tail[-1] is None:\n",
    "                continue\n",
    "            yield head + skip_tail\n",
    "\n",
    "sent = \"Insurgents killed in ongoing fighting\".split()\n",
    "list(skipgrams(sent, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bigram_sent(sent):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    verb = tokenizer.tokenize(sent.lower())\n",
    "    filtered_words = [word for word in verb if word not in stopwords.words('english')]\n",
    "    bigrams_list = list(skipgrams(filtered_words,2,10))\n",
    "    return bigrams_list,verb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bigrams_count(recipes):\n",
    "    recipe_corpus = []\n",
    "   \n",
    "    for text in recipes:\n",
    "        text_join = ''.join(text['directions'])\n",
    "        result = ''.join(i for i in text_join if not i.isdigit())\n",
    "        recipe_corpus.append(result)\n",
    "    merged = list(itertools.chain(recipe_corpus))\n",
    "    all_bigrams = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for merge in merged:\n",
    "        tokens = tokenizer.tokenize(merge.lower())\n",
    "        tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "        bigrams_merge = list(skipgrams(tokens,2,10))#.split(' '),2,2)\n",
    "        for (x,y) in bigrams_merge:\n",
    "            all_bigrams.append((x,y))\n",
    "    return all_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('combine', 'parmesan'), ('combine', 'cheese'), ('combine', 'pepper'), ('combine', 'garlic'), ('combine', 'powder'), ('parmesan', 'cheese'), ('parmesan', 'pepper'), ('parmesan', 'garlic'), ('parmesan', 'powder'), ('cheese', 'pepper'), ('cheese', 'garlic'), ('cheese', 'powder'), ('pepper', 'garlic'), ('pepper', 'powder'), ('garlic', 'powder')]\n"
     ]
    }
   ],
   "source": [
    "pairs,verb = bigram_sent(sent)\n",
    "print pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bake in 350 degrees F (175 degrees C) oven for 15 minutes or until golden brown']\n"
     ]
    }
   ],
   "source": [
    "sent = ['bake in 350 degrees F (175 degrees C) oven for 15 minutes or until golden brown']\n",
    "print sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('combine', 'parmesan'),\n",
       " ('combine', 'cheese'),\n",
       " ('combine', 'pepper'),\n",
       " ('combine', 'garlic'),\n",
       " ('combine', 'powder')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_with_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_recipes = bigrams_count(recipes[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bake\n",
      "{'bake': {('bake', 'golden'): 0.02630698172568445, ('bake', 'degrees'): 0.17223375058571525, ('bake', 'f'): 0.08966463618716113, ('bake', 'brown'): 0.03156168418234152, ('bake', 'c'): 0.08705401968003212, ('bake', 'minutes'): 0.2136689202757882, ('bake', 'oven'): 0.07895441461945243}}\n",
      "('bake', 'golden')\n",
      "('bake', 'degrees')\n",
      "('bake', 'f')\n",
      "('bake', 'brown')\n",
      "('bake', 'c')\n",
      "('bake', 'minutes')\n",
      "('bake', 'oven')\n",
      "6.30268674243e-13\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "all_recipes_sorted = dict(Counter(all_recipes))\n",
    "verb_probas = dict()\n",
    "for s in sent:\n",
    "    pairs,verb = bigram_sent(s)\n",
    "    bigrams_with_verb = []\n",
    "    print verb\n",
    "    for pair in pairs:\n",
    "        if verb in pair:\n",
    "            bigrams_with_verb.append(pair)\n",
    "    bigram_probas = dict()\n",
    "    for (x,y) in bigrams_with_verb:\n",
    "        if (x,y) in all_recipes_sorted.keys():\n",
    "            bigram_probas[(x,y)] = all_recipes_sorted[(x,y)]/float(actions_dict_upd[verb])\n",
    "    verb_probas[verb] = bigram_probas\n",
    "    verb_pr = 1/float(actions_dict_upd[verb])\n",
    "    proba_sent = verb_pr\n",
    "    print verb_probas\n",
    "    for key in verb_probas[verb]:\n",
    "        if verb_probas[verb][key] > 0:\n",
    "            proba_sent = proba_sent * verb_probas[verb][key]\n",
    "            print key #+ ('bowl','bowl')\n",
    "    print proba_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.24801870859e-05\n",
      "{'place': {('place', 'ungreased'): 0.008899571261530466, ('place', 'cookie'): 0.02478238274652462, ('place', 'sheet'): 0.0223463687150838}}\n"
     ]
    }
   ],
   "source": [
    "proba_sent = verb_pr\n",
    "for key in verb_probas[verb]:\n",
    "    if verb_probas[verb][key] > 0.1:\n",
    "        proba_sent = proba_sent * verb_probas[verb][key]\n",
    "        print key\n",
    "print proba_sent\n",
    "print verb_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigrams_with_verb = []\n",
    "for pair in pairs:\n",
    "    if verb not in pair:\n",
    "        bigrams_with_verb.append(pair)\n",
    "bigram_probas = dict()\n",
    "for (x,y) in bigrams_with_verb:\n",
    "    if (x,y) in all_recipes_sorted.keys():\n",
    "        bigram_probas[(x,y)] = all_recipes_sorted[(x,y)]/float(410)\n",
    "verb_probas[verb] = bigram_probas\n",
    "verb_pr = 1/float(actions_dict_upd[verb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'combine': {('soda', 'bowl'): 0.7902439024390244,\n",
       "  ('soup', 'bowl'): 0.10975609756097561,\n",
       "  ('soup', 'soda'): 0.00975609756097561,\n",
       "  ('tomato', 'bowl'): 0.06585365853658537,\n",
       "  ('tomato', 'soda'): 0.012195121951219513,\n",
       "  ('tomato', 'soup'): 0.11951219512195121}}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((u'combine', u'sugar'), 0.09327397554172816)\n",
      "((u'mix', u'well'), 0.10216435857036596)\n",
      "((u'stir', u'mixture'), 0.04374850585703928)\n",
      "((u'fry', u'brown'), 0.08408953418027812)\n",
      "((u'cool', u'wire'), 0.050940487203206396)\n",
      "((u'boil', u'minutes'), 0.46467817896388086)\n",
      "((u'place', u'minutes'), 0.04043783292191641)\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "\n",
    "for verb in verbs:\n",
    "    scores = {}\n",
    "    for (x,y) in all_recipes:\n",
    "        if x == verb and y not in utensils:\n",
    "            if (x,y) in scores:\n",
    "                scores[(x,y)] += 1/float(actions_dict_upd[verb])\n",
    "            else:\n",
    "                scores[(x,y)] = 1/float(actions_dict_upd[verb])\n",
    "    print max(scores.iteritems(), key=operator.itemgetter(1))[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(u'mix', u'bowl'): 656,\n",
       " (u'mix', u'dish'): 302,\n",
       " (u'mix', u'heat'): 394,\n",
       " (u'mix', u'oven'): 301,\n",
       " (u'mix', u'pan'): 684,\n",
       " (u'mix', u'saucepan'): 101,\n",
       " (u'mix', u'skillet'): 87}"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29878"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_dict_upd['bake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utensils = ['bowl','oven','pan','saucepan','heat','dish','skillet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs = ['combine','mix','stir','fry','cool','boil','place']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
